#
# Copyright Â© 2016, Evolved Binary Ltd
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are met:
#     * Redistributions of source code must retain the above copyright
#       notice, this list of conditions and the following disclaimer.
#     * Redistributions in binary form must reproduce the above copyright
#       notice, this list of conditions and the following disclaimer in the
#       documentation and/or other materials provided with the distribution.
#     * Neither the name of the <organization> nor the
#       names of its contributors may be used to endorse or promote products
#       derived from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
# WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
# DISCLAIMED. IN NO EVENT SHALL <COPYRIGHT HOLDER> BE LIABLE FOR ANY
# DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
# (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
# ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
# SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
#

import argparse

import numpy as np
import matplotlib.pyplot as plt
import os
import pandas as pd


def plot_byte_array_from_native_performance_comparisons(results_dict, chart_title_template):
    for k, benchmarks in results_dict.items():
        fig = plt.figure(num=None, figsize=(12, 8), dpi=80,
                         facecolor='w', edgecolor='k')
        ax1 = fig.add_subplot(111)
        for name, obj in benchmarks.items():
            samples = obj["scores"]
            errors = obj["errors"]
            x = np.linspace(0, len(samples), len(samples))
            ax1.errorbar(x, samples, yerr=errors, fmt='-o', label=name)
            ax1.legend()
        plt.title(chart_title_template.format(str(k)))
        plt.xlabel("Sample")
        plt.ylabel("Time [ns]")
        plt.savefig("fig" + str(k) + ".png")

# Columns:
# Benchmark	Mode	Threads	Samples	Score	Score Error (99.9%)	Unit	Param: valueSize


def process_value_results(path, param_name, chart_title_template):
    # Example results_dict: { 10: { "benchmark1": { "scores": [223, 243, 221, 219], "errors": [22, 25, 12, 45]}, "benchmark2": { "scores": [566, ...], "errors": [22, ...] }, ... }, 50: { ... }, ... }
    results_dict = {}
    for file in os.listdir(path):
        if file.endswith(".csv"):
            fp = os.path.join(path, file)
            print(fp)
            df = pd.read_csv(fp)
            df = df.iloc[::9, :]
            df["Benchmark"] = df["Benchmark"].apply(lambda x: x.split('.')[-1])
            unique_sizes = df[param_name].unique().tolist()
            for sz in unique_sizes:
                if sz not in results_dict:
                    results_dict[sz] = {}
                df_for_sz = df[df[param_name] == sz]
                for index, row in df_for_sz.iterrows():
                    if row["Benchmark"] not in results_dict[sz]:
                        results_dict[sz][row["Benchmark"]] = {
                            "scores": [], "errors": []}
                    results_dict[sz][row["Benchmark"]
                                     ]["scores"].append(row["Score"])
                    results_dict[sz][row["Benchmark"]]["errors"].append(
                        row["Score Error (99.9%)"])

    plot_byte_array_from_native_performance_comparisons(
        results_dict, chart_title_template)


# Example usage:
# python process_byte_array_benchmarks_results.py -p results_dir/ --param-name "Param: valueSize" --chart-title "Performance comparison of getting byte array with {} bytes via JNI"
def main():
    parser = argparse.ArgumentParser(
        description='Process JMH benchmarks result files (only SampleTime mode supported).')
    parser.add_argument('-p', '--path', type=str,
                        help='Path to the directory with benchmarking results generated by JMH run', default='/Users/alan/swProjects/evolvedBinary/jni-benchmarks/plotthis/')
    parser.add_argument('--param-name', type=str,
                        help='Benchmarks parameter name', default='Param: valueSize')
    parser.add_argument('--chart-title', type=str, help='Charts\' title',
                        default='Performance comparison of getting byte array with {} bytes via JNI')
    args = parser.parse_args()
    process_value_results(args.path, args.param_name, args.chart_title)


if __name__ == "__main__":
    main()
